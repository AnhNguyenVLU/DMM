{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anhng\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c12e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68acfb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "078660ae",
   "metadata": {},
   "source": [
    "Find a dataset about Python(or similar; not too big) - you guys can search code parrot by hugging face....(https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text) they have dataset for python - donâ€™t take everything - it will be too big"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc1af9ba",
   "metadata": {},
   "source": [
    "# 1. Load data - Wiki Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd13d9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/anhng/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (C:/Users/anhng/.cache/huggingface/datasets/codeparrot___parquet/codeparrot--github-jupyter-code-to-text-cf9b56d996fd17e1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repo_name', 'path', 'license', 'content'],\n",
      "    num_rows: 47452\n",
      "}) Dataset({\n",
      "    features: ['repo_name', 'path', 'license', 'content'],\n",
      "    num_rows: 11864\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets \n",
    "train = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"train\")\n",
    "test  = datasets.load_dataset(\"codeparrot/github-jupyter-code-to-text\", split=\"test\")\n",
    "print(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beba6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split New line Sentence\n",
    "train_split = [split for text in train['content'] for split in text.split('\\n') if split != \"\"]\n",
    "test_split = [split for text in test['content'] for split in text.split('\\n') if split != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2aac4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11367363, 2875424)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_split), len(test_split)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd7b5d1d",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecce5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokenized_dataset_train = yield_tokens(train_split[:int(len(train_split)/100)])\n",
    "tokenized_dataset_test = yield_tokens(test_split[:int(len(test_split)/100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b41a0252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocessing(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = {'a', 'an', 'and', 'the', 'this', 'that', 'is'}\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the words\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        if word.endswith('s'):\n",
    "            word = word[:-1]  # remove plural 's'\n",
    "        lemmatized_words.append(word)\n",
    "    \n",
    "    # Join the words back into a string\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d42a999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 2956\n",
      "['<unk>', '<eos>', 'of', 'explanation', 'to', 'in', 'for', 'end', 'a', 'value']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for text in data_iter:\n",
    "        yield tokenizer(preprocessing(text))\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_split[:int(len(test_split)/100)]), min_freq=5) \n",
    "vocab.insert_token('<unk>', 0)           \n",
    "vocab.insert_token('<eos>', 1)            \n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print('Vocab Size',len(vocab))                         \n",
    "print(vocab.get_itos()[:10])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d96a5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Vocab Size check 2956\n"
     ]
    }
   ],
   "source": [
    "with open('vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in vocab.get_itos():\n",
    "        f.write(\"%s\\n\" % item)\n",
    "    print('Done')\n",
    "\n",
    "v = [line.rstrip() for line in open('vocab.txt', mode = 'r', encoding='utf-8')]\n",
    "print('Vocab Size check', len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6835fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# Store data (serialize)\n",
    "with open('vocab.pickle', 'wb') as handle:\n",
    "    pickle.dump(vocab, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Load data (deserialize)\n",
    "with open('vocab.pickle', 'rb') as handle:\n",
    "    check_vocab = pickle.load(handle)\n",
    "check_vocab #Good"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baf74f69",
   "metadata": {},
   "source": [
    "# 3. Prepare the batch loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f703c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:       \n",
    "        #appends eos so we know it ends....so model learn how to end...                             \n",
    "        tokens = example.append('<eos>') #end of sentence\n",
    "        #numericalize          \n",
    "        tokens = [vocab[token] for token in example] \n",
    "        data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    data = data[:num_batches * batch_size]                       \n",
    "    data = data.view(batch_size, num_batches)        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1282b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_data = get_data(tokenized_dataset_train, vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset_test, vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5311613b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 16592])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape #[batch_size, all the next length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d280c80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e2aacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, hid_dim, num_layers, dropout_rate):\n",
    "                \n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.positional_encoding = PositionalEncoding(emb_dim, max_len=512)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(hid_dim, dropout_rate) for _ in range(num_layers)])\n",
    "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        # There is no hidden state in the Transformer, so we return None\n",
    "        return None\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # src: [batch_size, src_len]\n",
    "        # tgt: [batch_size, tgt_len]\n",
    "        # src_mask: [batch_size, 1, 1, src_len]\n",
    "        # tgt_mask: [batch_size, 1, tgt_len, tgt_len]\n",
    "\n",
    "        embedded_tgt = self.embedding(tgt)\n",
    "        # embedded_tgt: [batch_size, tgt_len, emb_dim]\n",
    "\n",
    "        # Apply positional encoding\n",
    "        embedded_tgt = self.positional_encoding(embedded_tgt)\n",
    "\n",
    "        # Apply the decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            embedded_tgt = layer(embedded_tgt, src_mask, tgt_mask)\n",
    "\n",
    "        # Apply the final linear layer to get the predicted next word\n",
    "        prediction = self.fc(embedded_tgt)\n",
    "        # prediction: [batch_size, tgt_len, vocab_size]\n",
    "\n",
    "        return prediction, None\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(hid_dim, num_heads=8, dropout=dropout_rate)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 4*hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*hid_dim, hid_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(hid_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask):\n",
    "        # tgt: [batch_size, tgt_len, hid_dim]\n",
    "        # memory: [batch_size, src_len, hid_dim]\n",
    "        # tgt_mask: [batch_size, tgt_len, tgt_len]\n",
    "\n",
    "        # Apply self-attention with dropout and residual connection\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.layer_norm1(tgt)\n",
    "\n",
    "        # Apply encoder-decoder attention with dropout and residual connection\n",
    "        tgt2, _ = nn.MultiheadAttention(hid_dim, num_heads=8, dropout=dropout_rate)(tgt, memory, memory, attn_mask=None)\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.layer_norm2(tgt)\n",
    "\n",
    "        # Apply feedforward with dropout and residual connection\n",
    "        tgt2 = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout(tgt2)\n",
    "        tgt = self.layer_norm2(tgt)\n",
    "\n",
    "        return tgt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7861f87b",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e035d081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 309,719,948 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 512                \n",
    "hid_dim = 2048               \n",
    "num_layers = 6              \n",
    "dropout_rate = 0.1              \n",
    "lr = 1e-4 \n",
    "import torch.optim as optim\n",
    "decoder = TransformerDecoder(vocab_size, emb_dim, hid_dim, num_layers, dropout_rate).to(device)\n",
    "optimizer = optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e33e40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fed76312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    src = data[:, idx:idx+seq_len]\n",
    "    target = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0507013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #trg    = [batch size, trg len]\n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg    = trg[:,1:].reshape(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1519369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, batch_size, seq_len, device):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, trg in loader:\n",
    "\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "\n",
    "            #trg    = [batch size, trg len]\n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg    = trg[:,1:].reshape(-1)\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265a79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19dfc69c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     12\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 14\u001b[0m     train_loss \u001b[39m=\u001b[39m train(decoder, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n\u001b[0;32m     15\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(decoder, valid_data, criterion, batch_size, seq_len, device)\n\u001b[0;32m     17\u001b[0m     \u001b[39m# for plotting\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'seq_len' is not defined"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 5\n",
    "clip = 1\n",
    "\n",
    "save_path = f'{decoder.__class__.__name__}_general.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(decoder, train_data, optimizer, criterion, batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(decoder, valid_data, criterion, batch_size, seq_len, device)\n",
    "\n",
    "    # for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(decoder.state_dict(), save_path)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\tVal. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41032f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "deep learning model will be used to open a model that can be used to classify all of these data to a model that will be used to calculate a dataset in a\n",
      "\n",
      "0.7\n",
      "deep learning model should be able to use a pretty good department or a particular model that would be used to optimize your model that you need to recover a small distribution of\n",
      "\n",
      "0.75\n",
      "deep learning model should be able to use one form which can be used by a powerful department or just more or more to 1 on a small group of individual science or\n",
      "\n",
      "0.8\n",
      "deep learning model should be able to use open first movie or equal than more or low than 1 or more likely than one sample with every pair of a line is being\n",
      "\n",
      "1.0\n",
      "deep learning model should be able to use open 70 to not free efficiently division dimension or high or white estimation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'deep learning model'\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "            #superdiverse   more diverse\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0] \n",
    "#sample from this distribution higher probability will get more change\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec0ae0345fcfd632db8f4368261e96c9db5bfd3c141106930369a7b5fd0c8db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
