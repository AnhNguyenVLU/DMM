{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90bbe30-b3bf-4d0e-978f-c04fecade210",
   "metadata": {},
   "source": [
    "# Improved Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c669493-03e4-4bde-be9f-ff0f1c83b6f1",
   "metadata": {},
   "source": [
    "<b>Name:</b> Abhinav Lugun  <b>Student Id:</b> st122322"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d48812-b15d-4818-b760-c9275d0d97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext, datasets, math\n",
    "from torchtext.vocab import vocab as torchTextVocab\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import random, math, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eecd490-74e3-40aa-b685-389edd7979a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7c88f-9b1b-4e44-ac86-0deedcf84e6c",
   "metadata": {},
   "source": [
    "# 1) Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1667ed08-abf9-4537-ad48-a992bfa4c6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lvwerra--codeparrot-clean-fb728533b9673c8b\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"lvwerra/codeparrot-clean\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bad96f0-5b74-4526-9698-5af16578a935",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dataset = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "087f702d-f20d-43e3-bc82-1d911d0279fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['repo_name',\n",
       " 'path',\n",
       " 'copies',\n",
       " 'size',\n",
       " 'content',\n",
       " 'license',\n",
       " 'hash',\n",
       " 'line_mean',\n",
       " 'line_max',\n",
       " 'alpha_frac',\n",
       " 'autogenerated']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(next(iter_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f94f57-f02b-4eaf-8e35-6e57357b4110",
   "metadata": {},
   "source": [
    "# 2) Extracting data related to pytorch code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5dd43-740b-4248-a509-9a788ac73441",
   "metadata": {},
   "source": [
    "All repo containing Pytorch code will be extracted since the main emphasis is given for suggesting Pytorch code for code autocomplete. Since the repo also contain other parts related to such as numpy, and matplotlib, the model will learn to suggest code completion for those parts also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26d18872-4dc6-4f77-b576-81e71d4904de",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_related = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb581de0-6da8-4711-adb7-6c45c18d1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "threshold = 200\n",
    "count = 0\n",
    "\n",
    "for repo in iter_dataset:\n",
    "    if 'torch' in repo['repo_name']:\n",
    "        pytorch_related.append(repo['content'])\n",
    "        count += 1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        \n",
    "        if count == threshold:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeb3b9c8-81de-4b47-aebb-69fa5831aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import unittest\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import pipeline\n",
      "from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow\n",
      "\n",
      "from .test_pipelines_common import MonoInputPipelineCommonMixin\n",
      "\n",
      "\n",
      "if is_torch_available():\n",
      "    from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration\n",
      "\n",
      "\n",
      "class TranslationEnToDePipelineTests(MonoInputPipelineCommonMixin, unittest.TestCase):\n",
      "    pipeline_task = \"translation_en_to_de\"\n",
      "    small_mo\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print(pytorch_related[idx][:1100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320220d-116c-42e9-a961-215602b70547",
   "metadata": {},
   "source": [
    "### Splitting into train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6373b56-a048-4b7e-90e0-7d1e4a35cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Len = len(pytorch_related)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "808d2407-58c4-4f08-9a92-e092e298d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end   = int(0.7 * Len)\n",
    "train = pytorch_related[:end]\n",
    "\n",
    "start = int(0.7 * Len)\n",
    "end   = int(0.8 * Len)\n",
    "valid = pytorch_related[start: end]\n",
    "\n",
    "start = int(0.8 * Len)\n",
    "test  = pytorch_related[start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdfaecf-8862-46a5-93db-81bf76b11be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 20, 40)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ee0c54-cb0d-4cc5-9ec6-d9e23ae7ab43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import unittest\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import pipeline\n",
      "from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow\n",
      "\n",
      "from .test_pipelines_common import MonoInputPipelineCommonMixin\n",
      "\n",
      "\n",
      "if is_torch_available():\n",
      "    from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration\n",
      "\n",
      "\n",
      "class TranslationEnToDePipelineTests(MonoInputPipelineCommonMixin, unittest.TestCase):\n",
      "    pipeline_task = \"translation_en_to_de\"\n",
      "    small_models = [\"patrickvonplaten/t5-tiny-random\"]  # Default model - Models tested without the @slow decorator\n",
      "    large_models = [None]  # Models tested with the @slow decorator\n",
      "    invalid_inputs = [4, \"<m\n"
     ]
    }
   ],
   "source": [
    "print(train[0][:1300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "967a21f5-70da-46dd-b8fa-1fac794ef4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdata.datapipes.iter import IterableWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c7b478-e835-4147-a47b-611d56117bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = IterableWrapper(train)\n",
    "valid = IterableWrapper(valid)\n",
    "test  = IterableWrapper(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94796384-4437-4943-8849-a6884173c911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 20, 40)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(iter(train))), len(list(iter(valid))), len(list(iter(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6633ea9-8f39-4c95-9086-80135e9602fd",
   "metadata": {},
   "source": [
    "# 3) Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c595af09-38e1-4aa8-a569-65ed963f818b",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2f29fa-7520-4cb0-ac50-53349e00da1e",
   "metadata": {},
   "source": [
    "To tokenize Python code accurately, 'tokenize' module was used. It is meant to provide a lexical scanner for Python code. However, in this case, the module was extremely useful since apart from returning tokens, it also return its type which made it possible to treadtnumbers and strings alike. Also during tokenizing, comments were excluded since they are not useful for training the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57a24b-4ef3-43d4-a8da-e44ebb05b692",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/62166362/how-to-tokenize-python-code-using-the-tokenize-module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f6cfec1-f4c0-4e54-9ed9-b805097361d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenize\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c669b9c4-1210-47da-86c8-fe0bed0aaaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_name = tokenize.tok_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcf65397-5745-4fa0-b909-43ef68a58124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'ENDMARKER', 1: 'NAME', 2: 'NUMBER', 3: 'STRING', 4: 'NEWLINE', 5: 'INDENT', 6: 'DEDENT', 7: 'LPAR', 8: 'RPAR', 9: 'LSQB', 10: 'RSQB', 11: 'COLON', 12: 'COMMA', 13: 'SEMI', 14: 'PLUS', 15: 'MINUS', 16: 'STAR', 17: 'SLASH', 18: 'VBAR', 19: 'AMPER', 20: 'LESS', 21: 'GREATER', 22: 'EQUAL', 23: 'DOT', 24: 'PERCENT', 25: 'LBRACE', 26: 'RBRACE', 27: 'EQEQUAL', 28: 'NOTEQUAL', 29: 'LESSEQUAL', 30: 'GREATEREQUAL', 31: 'TILDE', 32: 'CIRCUMFLEX', 33: 'LEFTSHIFT', 34: 'RIGHTSHIFT', 35: 'DOUBLESTAR', 36: 'PLUSEQUAL', 37: 'MINEQUAL', 38: 'STAREQUAL', 39: 'SLASHEQUAL', 40: 'PERCENTEQUAL', 41: 'AMPEREQUAL', 42: 'VBAREQUAL', 43: 'CIRCUMFLEXEQUAL', 44: 'LEFTSHIFTEQUAL', 45: 'RIGHTSHIFTEQUAL', 46: 'DOUBLESTAREQUAL', 47: 'DOUBLESLASH', 48: 'DOUBLESLASHEQUAL', 49: 'AT', 50: 'ATEQUAL', 51: 'RARROW', 52: 'ELLIPSIS', 53: 'COLONEQUAL', 54: 'OP', 55: 'AWAIT', 56: 'ASYNC', 57: 'TYPE_IGNORE', 58: 'TYPE_COMMENT', 59: 'ERRORTOKEN', 60: 'COMMENT', 61: 'NL', 62: 'ENCODING', 63: 'N_TOKENS', 256: 'NT_OFFSET'}\n"
     ]
    }
   ],
   "source": [
    "print(tok_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9e1e637-1647-4c68-b8f6-0447abb111f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_code_tokenizer(content):\n",
    "    tokenized_code = []\n",
    "    \n",
    "    try:\n",
    "        for token in tokenize.generate_tokens(io.StringIO(content).readline):\n",
    "            encoding = tok_name[token.type]\n",
    "            line = token.line\n",
    "            if line == '':\n",
    "                continue\n",
    "            \n",
    "            if encoding == \"COMMENT\" or encoding== \"NL\":\n",
    "                continue\n",
    "            elif encoding == \"NUMBER\":\n",
    "                tokenized_code.append(\"<NUMBER>\")\n",
    "            elif encoding == \"STRING\":\n",
    "                tokenized_code.append(\"<STRING>\")\n",
    "            else:\n",
    "                tokenized_code.append(token.string)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    return tokenized_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab238509-cf7f-4404-b133-5f4338f070e4",
   "metadata": {},
   "source": [
    "#### Testing python_code_tokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8caa888-3e82-4cb2-b097-c1b233ccb893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import unittest\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import pipeline\n",
      "from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow\n",
      "\n",
      "from .test_pipelines_common import MonoInputPipelineCommonMixin\n",
      "\n",
      "\n",
      "if is_torch_available():\n",
      "    from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration\n",
      "\n",
      "\n",
      "class TranslationEnToDePipelineTests(MonoI\n"
     ]
    }
   ],
   "source": [
    "## Sample before tokenization \n",
    "\n",
    "sample = next(iter(train))\n",
    "print(sample[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fd370a0-1617-4c08-8bfb-3e1b0dbcb288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', 'unittest', '\\n', 'import', 'pytest', '\\n', 'from', 'transformers', 'import', 'pipeline', '\\n', 'from', 'transformers', '.', 'testing_utils', 'import', 'is_pipeline_test', ',', 'is_torch_available', ',', 'require_torch', ',', 'slow', '\\n', 'from', '.', 'test_pipelines_common', 'import', 'MonoInputPipelineCommonMixin', '\\n', 'if', 'is_torch_available', '(', ')', ':', '\\n', '    ', 'from', 'transformers', '.', 'models', '.', 'mbart', 'import', 'MBart50TokenizerFast', ',', 'MBartForConditionalGeneration', '\\n', '', 'class']\n"
     ]
    }
   ],
   "source": [
    "## Sample after tokenization\n",
    "tokenized_sample = python_code_tokenizer(sample)\n",
    "print(tokenized_sample[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d70ee9-1540-46f3-bd26-f82f179afdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79b95d80-48eb-4ab3-9347-53961cac62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data):\n",
    "    for data_sample in data:\n",
    "        yield python_code_tokenizer(data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9f5b1-479a-4f19-8dcd-d964cbd6424b",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc6f3d41-db1c-4b18-b2ab-713510fb4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edbca83-37c1-4130-82ce-6ef86ce8d028",
   "metadata": {},
   "source": [
    "## Text to integers (Numericalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe8c8e-a4e1-4b42-8f82-7a92a020b8a5",
   "metadata": {},
   "source": [
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7774070c-42a3-4090-af22-e3ea31571692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# Create torchtext's Vocab object \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), \n",
    "                                                min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                specials=special_symbols,\n",
    "                                                special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31c1b301-5cc4-4269-a920-4790221fa87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[22, 271, 68, 74]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab(['import', 'numpy', 'as', 'np'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40daaef1-7d21-4aba-8736-a82e4ad81762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab.get_itos()\n",
    "\n",
    "#print 22, for example\n",
    "mapping[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c468a83-405a-491a-b72d-3ca5900f6316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57de3a71-fff6-4016-af62-7860572b9923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc2e709b-da20-485b-b4df-e3f905a206bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5490"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2c62928-213e-4191-b4a6-680afc9b2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vocab, 'vocab_obj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f071df1c-e176-4f1b-a4b9-1c2bfe80b3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_obj = torch.load('vocab_obj.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecff6bf7-0a7a-4732-be1b-378ff136e509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import unittest\n",
      "\n",
      "import pytest\n",
      "\n",
      "from transformers import pipeline\n",
      "from transformers.testing_utils import is_pipeline_test, is_torch_available, require_torch, slow\n",
      "\n",
      "from .test_pipelines_common import MonoInputPipelineCommonMixin\n",
      "\n",
      "\n",
      "if is_torch_available():\n",
      "    from transformers.models.mbart import MBart50TokenizerFast, MBartForConditionalGeneration\n",
      "\n",
      "\n",
      "class TranslationEnToDePipelineTests(MonoI\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train))\n",
    "print(sample[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc0c3de-3d03-450a-9f96-82a602450567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4114f79b-82fe-4f54-ad0f-93348afcdec9",
   "metadata": {},
   "source": [
    "# 4) Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93cc2a4a-88e7-40de-a460-0aa4ec5e9083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2020 The HuggingFace Team. All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, V\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train))\n",
    "print(sample[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69a6bf57-0cf8-4e2e-bc5b-87078e27834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e79d515e-e165-4ae1-af33-d5ee5e0410fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(data):\n",
    "    pairs = []\n",
    "    \n",
    "    for end_idx in range(1, len(data) - 1):\n",
    "        start_idx = end_idx - seq_len\n",
    "        \n",
    "        if start_idx < 0:\n",
    "            start_idx = 0\n",
    "            \n",
    "        src = data[start_idx:end_idx]\n",
    "        trg = data[start_idx + 1:end_idx + 1] # trg simply is ahead of src by 1   \n",
    "        \n",
    "        \n",
    "    return src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47d99349-e73e-4e07-8e46-0c49576fe2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = sequential_transforms(python_code_tokenizer, #Tokenization\n",
    "                                               vocab, #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    \n",
    "    for sample in batch:\n",
    "        processed_code = text_transform(sample)\n",
    "        src, trg = get_pair(processed_code)\n",
    "        src_batch.append(src)\n",
    "        trg_batch.append(trg)\n",
    "        src_len_batch.append(len(src))\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a131886-6b22-42fa-875a-0b749090f961",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec2c4029-6c0f-4941-959e-dfccac2e5092",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\graph_settings.py:74: UserWarning: `shuffle=True` was set, but the datapipe does not contain a `Shuffler`. Adding one at the end. Be aware that the default buffer size might not be sufficient for your task.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b00ace0-9ad0-4a63-b185-9bd697a9d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src, seq_length, trg in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4de6ca91-5b57-4002-9352-f70777023f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src shape:  torch.Size([64, 20])\n",
      "seq_len shape:  torch.Size([64])\n",
      "trg shape:  torch.Size([64, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"src shape: \", src.shape)  # (batch_size, seq len)\n",
    "print(\"seq_len shape: \", seq_length.shape)   # (batch_size)\n",
    "print(\"trg shape: \", trg.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2647e-6fa7-48ad-8b57-0659759ba10e",
   "metadata": {},
   "source": [
    "# 5) Design the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675503a2-f6d0-4e87-a0bc-2f64dca15ab9",
   "metadata": {},
   "source": [
    "## Mutli Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cd4e9d4-d867-48cc-a59c-1e8043256926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads  #make sure it's divisible....\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc   = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale   = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, q, k, v, mask = None):\n",
    "        b = q.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(q)\n",
    "        K = self.fc_k(k)\n",
    "        V = self.fc_v(v)\n",
    "        #Q, K, V = [b, l, h]\n",
    "        \n",
    "        #reshape them into head_dim\n",
    "        #reshape them to [b, n_heads, l, head_dim]\n",
    "        Q = Q.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(b, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q, K, V = [b, n_heads, l, head_dim]\n",
    "        \n",
    "        #e = QK/sqrt(dk)\n",
    "        e = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #e: [b, n_heads, ql, kl]\n",
    "        \n",
    "        if mask is not None:\n",
    "            e = e.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        a = torch.softmax(e, dim=-1)\n",
    "        \n",
    "        #eV\n",
    "        x = torch.matmul(self.dropout(a), V)\n",
    "        #x: [b, n_heads, ql, head_dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x: [b, ql, n_heads, head_dim]\n",
    "        \n",
    "        #concat them together\n",
    "        x = x.view(b, -1, self.hid_dim)\n",
    "        #x: [b, ql, h]\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        #x = [b, ql, h]\n",
    "        \n",
    "        return x, a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c37f2-9017-4b45-afbc-745f368305b2",
   "metadata": {},
   "source": [
    "## Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ea4cff1-3c5e-408b-80fb-7d5d72875beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.dropout(torch.relu(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa72ba5-f097-4e51-9458-26af7c36a3d2",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0e933ece-f342-45dc-a7fb-7e15f94da81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, trg_pad_idx, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.trg_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "        \n",
    "    def forward(self, trg):\n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.trg_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "                \n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, trg_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a69a2054-ebbc-42ce-bcfe-d50642b5f7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Decoder(nn.Module):\n",
    "    \n",
    "#     def __init__(self, output_dim, hid_dim, n_layers, n_heads,\n",
    "#                  pf_dim, dropout, src_pad_idx, trg_pad_idx, device,\n",
    "#                  max_length = 100):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.pos_emb = nn.Embedding(max_length, hid_dim)\n",
    "#         self.trg_emb = nn.Embedding(output_dim, hid_dim)\n",
    "        \n",
    "#         self.src_pad_idx = src_pad_idx\n",
    "#         self.trg_pad_idx = trg_pad_idx\n",
    "#         self.scale   = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.layers  = nn.ModuleList(\n",
    "#                      [\n",
    "#                         DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "#                         for _ in range(n_layers)\n",
    "#                      ]\n",
    "#                      )\n",
    "#         self.fc      = nn.Linear(hid_dim, output_dim)\n",
    "#         self.device  = device\n",
    "    \n",
    "#     def make_src_mask(self, src):\n",
    "        \n",
    "#         #src = [batch size, src len]\n",
    "        \n",
    "#         src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#         #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "#         return src_mask\n",
    "    \n",
    "#     def make_trg_mask(self, trg):\n",
    "        \n",
    "#         #trg = [batch size, trg len]\n",
    "        \n",
    "#         trg_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "#         #trg_mask: [b, 1, 1, trg len]\n",
    "        \n",
    "#         trg_len = trg_mask.shape[-1]\n",
    "        \n",
    "#         trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "#         #trg_sub_mask = [trg len, trg len]\n",
    "        \n",
    "#         trg_mask = trg_mask & trg_sub_mask\n",
    "#         #trg_mask: [b, 1, trg len, trg len]\n",
    "        \n",
    "#         return trg_mask\n",
    "\n",
    "#     def forward(self, trg, src):\n",
    "#         #src = [b, sl]\n",
    "#         #trg = [b, tl]\n",
    "        \n",
    "#         b = trg.shape[0]\n",
    "#         l = trg.shape[1]\n",
    "        \n",
    "#         src_mask = self.make_src_mask(src)\n",
    "#         trg_mask = self.make_trg_mask(trg)\n",
    "#         #src_mask = [b, 1, 1, sl]\n",
    "#         #trg_mask = [b, 1, tl, tl]\n",
    "        \n",
    "#         #pos\n",
    "#         pos = torch.arange(0, l).unsqueeze(0).repeat(b, 1).to(self.device)\n",
    "#         #pos: [b, l]\n",
    "        \n",
    "#         pos_emb = self.pos_emb(pos) #[b, l, h]\n",
    "#         trg_emb = self.trg_emb(trg) #[b, l, h]\n",
    "        \n",
    "#         x = pos_emb + trg_emb * self.scale #[b, l, h]\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         for layer in self.layers:\n",
    "#             trg, attention = layer(x, trg_mask, src_mask)\n",
    "        \n",
    "#         output = self.fc(trg)\n",
    "        \n",
    "#         return output, attention\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a060420-0c7b-436a-a48f-07d531a00f1d",
   "metadata": {},
   "source": [
    "## Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1d27c40d-7d6b-4ff5-8296-de4a2fd743e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "#         self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "#         self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "#         self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, trg, trg_mask, src_mask):\n",
    "        \n",
    "#         #trg = [batch size, trg len, hid dim]\n",
    "#         #trg_mask = [batch size, 1, trg len, trg len]\n",
    "#         #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "#         #self attention\n",
    "#         _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "#         #dropout, residual connection and layer norm\n",
    "#         trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "#         #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "#         #positionwise feedforward\n",
    "#         _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "#         #dropout, residual and layer norm\n",
    "#         trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "#         #trg = [batch size, trg len, hid dim]\n",
    "#         #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "#         return trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b6527fc-d6fb-4743-8905-1800d6bee652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, trg_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99810760-4bc8-4ff2-8b64-f043cb85fb0f",
   "metadata": {},
   "source": [
    "# 6) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176ee14-eb4d-4491-be08-b3fe5639bfaa",
   "metadata": {},
   "source": [
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly.\n",
    "\n",
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d7aee9a3-6ae2-4210-ad16-ffe04ec56f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ec13d472-8d4b-419b-948f-0526816c2378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (trg_embedding): Embedding(5490, 256)\n",
       "  (pos_embedding): Embedding(100, 256)\n",
       "  (layers): ModuleList(\n",
       "    (0): DecoderLayer(\n",
       "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): DecoderLayer(\n",
       "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): DecoderLayer(\n",
       "      (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (self_attention): MultiHeadAttentionLayer(\n",
       "        (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "        (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc_out): Linear(in_features=256, out_features=5490, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM = len(vocab)\n",
    "HID_DIM = 256\n",
    "DEC_LAYERS = 3\n",
    "DEC_HEADS = 8\n",
    "DEC_PF_DIM = 512\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "model = Decoder(OUTPUT_DIM, HID_DIM, DEC_LAYERS, DEC_HEADS, \n",
    "              DEC_PF_DIM, DEC_DROPOUT, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c177404-39ac-46b8-ba94-c27d4c689d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1405440\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1405440\n",
      "  5490\n",
      "______\n",
      "4423282\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7a6978f4-c0ae-4df4-a585-d74886095be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d7499dc-986d-4b46-8c04-fa0c4cd38d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos\n",
    "        output = model(src[:,:-1])  \n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1)    #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42a87d5b-5c53-4ee6-9b33-de5154cd1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output = model(src[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7b826-bc67-4499-b688-fd649e1ac45d",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75a9bd00-f614-452c-b604-9e0fb84f2b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd9a4754-7d35-4998-9049-2d4e42b31e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5a351f8-d2f5-4a7d-98ef-5fb2b0698a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 2s\n",
      "\tTrain Loss: 8.205 | Train PPL: 3657.509\n",
      "\t Val. Loss: 7.538 |  Val. PPL: 1878.587\n",
      "Epoch: 02 | Time: 0m 2s\n",
      "\tTrain Loss: 7.377 | Train PPL: 1599.384\n",
      "\t Val. Loss: 6.943 |  Val. PPL: 1035.734\n",
      "Epoch: 03 | Time: 0m 2s\n",
      "\tTrain Loss: 6.830 | Train PPL: 925.191\n",
      "\t Val. Loss: 6.446 |  Val. PPL: 630.274\n",
      "Epoch: 04 | Time: 0m 2s\n",
      "\tTrain Loss: 6.322 | Train PPL: 556.762\n",
      "\t Val. Loss: 5.977 |  Val. PPL: 394.375\n",
      "Epoch: 05 | Time: 0m 2s\n",
      "\tTrain Loss: 5.834 | Train PPL: 341.840\n",
      "\t Val. Loss: 5.537 |  Val. PPL: 254.036\n",
      "Epoch: 06 | Time: 0m 2s\n",
      "\tTrain Loss: 5.377 | Train PPL: 216.338\n",
      "\t Val. Loss: 5.142 |  Val. PPL: 171.118\n",
      "Epoch: 07 | Time: 0m 2s\n",
      "\tTrain Loss: 4.970 | Train PPL: 144.097\n",
      "\t Val. Loss: 4.806 |  Val. PPL: 122.210\n",
      "Epoch: 08 | Time: 0m 2s\n",
      "\tTrain Loss: 4.611 | Train PPL: 100.545\n",
      "\t Val. Loss: 4.532 |  Val. PPL:  92.970\n",
      "Epoch: 09 | Time: 0m 2s\n",
      "\tTrain Loss: 4.308 | Train PPL:  74.323\n",
      "\t Val. Loss: 4.315 |  Val. PPL:  74.792\n",
      "Epoch: 10 | Time: 0m 2s\n",
      "\tTrain Loss: 4.051 | Train PPL:  57.446\n",
      "\t Val. Loss: 4.142 |  Val. PPL:  62.903\n",
      "Epoch: 11 | Time: 0m 2s\n",
      "\tTrain Loss: 3.832 | Train PPL:  46.147\n",
      "\t Val. Loss: 4.006 |  Val. PPL:  54.927\n",
      "Epoch: 12 | Time: 0m 2s\n",
      "\tTrain Loss: 3.636 | Train PPL:  37.957\n",
      "\t Val. Loss: 3.907 |  Val. PPL:  49.738\n",
      "Epoch: 13 | Time: 0m 2s\n",
      "\tTrain Loss: 3.439 | Train PPL:  31.164\n",
      "\t Val. Loss: 3.830 |  Val. PPL:  46.046\n",
      "Epoch: 14 | Time: 0m 2s\n",
      "\tTrain Loss: 3.263 | Train PPL:  26.128\n",
      "\t Val. Loss: 3.792 |  Val. PPL:  44.357\n",
      "Epoch: 15 | Time: 0m 2s\n",
      "\tTrain Loss: 3.093 | Train PPL:  22.053\n",
      "\t Val. Loss: 3.720 |  Val. PPL:  41.254\n",
      "Epoch: 16 | Time: 0m 2s\n",
      "\tTrain Loss: 2.933 | Train PPL:  18.783\n",
      "\t Val. Loss: 3.675 |  Val. PPL:  39.463\n",
      "Epoch: 17 | Time: 0m 2s\n",
      "\tTrain Loss: 2.785 | Train PPL:  16.207\n",
      "\t Val. Loss: 3.666 |  Val. PPL:  39.081\n",
      "Epoch: 18 | Time: 0m 2s\n",
      "\tTrain Loss: 2.645 | Train PPL:  14.080\n",
      "\t Val. Loss: 3.638 |  Val. PPL:  37.997\n",
      "Epoch: 19 | Time: 0m 2s\n",
      "\tTrain Loss: 2.511 | Train PPL:  12.319\n",
      "\t Val. Loss: 3.625 |  Val. PPL:  37.523\n",
      "Epoch: 20 | Time: 0m 2s\n",
      "\tTrain Loss: 2.387 | Train PPL:  10.879\n",
      "\t Val. Loss: 3.621 |  Val. PPL:  37.365\n",
      "Epoch: 21 | Time: 0m 2s\n",
      "\tTrain Loss: 2.280 | Train PPL:   9.772\n",
      "\t Val. Loss: 3.620 |  Val. PPL:  37.335\n",
      "Epoch: 22 | Time: 0m 2s\n",
      "\tTrain Loss: 2.156 | Train PPL:   8.635\n",
      "\t Val. Loss: 3.638 |  Val. PPL:  38.015\n",
      "Epoch: 23 | Time: 0m 2s\n",
      "\tTrain Loss: 2.058 | Train PPL:   7.832\n",
      "\t Val. Loss: 3.618 |  Val. PPL:  37.278\n",
      "Epoch: 24 | Time: 0m 2s\n",
      "\tTrain Loss: 1.969 | Train PPL:   7.166\n",
      "\t Val. Loss: 3.619 |  Val. PPL:  37.317\n",
      "Epoch: 25 | Time: 0m 2s\n",
      "\tTrain Loss: 1.874 | Train PPL:   6.513\n",
      "\t Val. Loss: 3.603 |  Val. PPL:  36.723\n",
      "Epoch: 26 | Time: 0m 2s\n",
      "\tTrain Loss: 1.794 | Train PPL:   6.014\n",
      "\t Val. Loss: 3.632 |  Val. PPL:  37.792\n",
      "Epoch: 27 | Time: 0m 2s\n",
      "\tTrain Loss: 1.703 | Train PPL:   5.493\n",
      "\t Val. Loss: 3.635 |  Val. PPL:  37.911\n",
      "Epoch: 28 | Time: 0m 2s\n",
      "\tTrain Loss: 1.622 | Train PPL:   5.065\n",
      "\t Val. Loss: 3.665 |  Val. PPL:  39.072\n",
      "Epoch: 29 | Time: 0m 2s\n",
      "\tTrain Loss: 1.550 | Train PPL:   4.714\n",
      "\t Val. Loss: 3.692 |  Val. PPL:  40.144\n",
      "Epoch: 30 | Time: 0m 2s\n",
      "\tTrain Loss: 1.478 | Train PPL:   4.383\n",
      "\t Val. Loss: 3.687 |  Val. PPL:  39.921\n",
      "Epoch: 31 | Time: 0m 2s\n",
      "\tTrain Loss: 1.423 | Train PPL:   4.148\n",
      "\t Val. Loss: 3.729 |  Val. PPL:  41.648\n",
      "Epoch: 32 | Time: 0m 2s\n",
      "\tTrain Loss: 1.343 | Train PPL:   3.832\n",
      "\t Val. Loss: 3.730 |  Val. PPL:  41.682\n",
      "Epoch: 33 | Time: 0m 2s\n",
      "\tTrain Loss: 1.276 | Train PPL:   3.583\n",
      "\t Val. Loss: 3.741 |  Val. PPL:  42.141\n",
      "Epoch: 34 | Time: 0m 2s\n",
      "\tTrain Loss: 1.220 | Train PPL:   3.386\n",
      "\t Val. Loss: 3.788 |  Val. PPL:  44.172\n",
      "Epoch: 35 | Time: 0m 2s\n",
      "\tTrain Loss: 1.165 | Train PPL:   3.205\n",
      "\t Val. Loss: 3.779 |  Val. PPL:  43.785\n",
      "Epoch: 36 | Time: 0m 2s\n",
      "\tTrain Loss: 1.110 | Train PPL:   3.034\n",
      "\t Val. Loss: 3.815 |  Val. PPL:  45.361\n",
      "Epoch: 37 | Time: 0m 2s\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.878\n",
      "\t Val. Loss: 3.839 |  Val. PPL:  46.461\n",
      "Epoch: 38 | Time: 0m 2s\n",
      "\tTrain Loss: 1.005 | Train PPL:   2.731\n",
      "\t Val. Loss: 3.855 |  Val. PPL:  47.209\n",
      "Epoch: 39 | Time: 0m 2s\n",
      "\tTrain Loss: 0.958 | Train PPL:   2.607\n",
      "\t Val. Loss: 3.889 |  Val. PPL:  48.855\n",
      "Epoch: 40 | Time: 0m 2s\n",
      "\tTrain Loss: 0.906 | Train PPL:   2.474\n",
      "\t Val. Loss: 3.917 |  Val. PPL:  50.239\n",
      "Epoch: 41 | Time: 0m 2s\n",
      "\tTrain Loss: 0.873 | Train PPL:   2.395\n",
      "\t Val. Loss: 3.891 |  Val. PPL:  48.971\n",
      "Epoch: 42 | Time: 0m 2s\n",
      "\tTrain Loss: 0.822 | Train PPL:   2.274\n",
      "\t Val. Loss: 3.947 |  Val. PPL:  51.758\n",
      "Epoch: 43 | Time: 0m 2s\n",
      "\tTrain Loss: 0.785 | Train PPL:   2.192\n",
      "\t Val. Loss: 3.951 |  Val. PPL:  52.002\n",
      "Epoch: 44 | Time: 0m 2s\n",
      "\tTrain Loss: 0.743 | Train PPL:   2.103\n",
      "\t Val. Loss: 3.999 |  Val. PPL:  54.568\n",
      "Epoch: 45 | Time: 0m 2s\n",
      "\tTrain Loss: 0.717 | Train PPL:   2.049\n",
      "\t Val. Loss: 4.043 |  Val. PPL:  56.970\n",
      "Epoch: 46 | Time: 0m 2s\n",
      "\tTrain Loss: 0.676 | Train PPL:   1.967\n",
      "\t Val. Loss: 4.058 |  Val. PPL:  57.877\n",
      "Epoch: 47 | Time: 0m 2s\n",
      "\tTrain Loss: 0.647 | Train PPL:   1.910\n",
      "\t Val. Loss: 4.103 |  Val. PPL:  60.507\n",
      "Epoch: 48 | Time: 0m 2s\n",
      "\tTrain Loss: 0.621 | Train PPL:   1.860\n",
      "\t Val. Loss: 4.075 |  Val. PPL:  58.843\n",
      "Epoch: 49 | Time: 0m 2s\n",
      "\tTrain Loss: 0.577 | Train PPL:   1.782\n",
      "\t Val. Loss: 4.095 |  Val. PPL:  60.013\n",
      "Epoch: 50 | Time: 0m 2s\n",
      "\tTrain Loss: 0.563 | Train PPL:   1.756\n",
      "\t Val. Loss: 4.102 |  Val. PPL:  60.464\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 50\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "312f17ad-076a-428f-b8f5-d0ac306af6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bd0b205b-ca08-4e16-bf73-74382314c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.656 | Test PPL:  38.697 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0e64ef-5260-47c1-9148-eefa2ac7f93a",
   "metadata": {},
   "source": [
    "# 7) Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "123bfd95-85df-4b1d-bbef-aba1a84d04f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"import numpy as\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3683f747-b74a-4600-b21d-e44a8294a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 53,  3], device='cuda:0')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = text_transform(sample[0]).to(device)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71c6f2eb-66d3-469d-abcc-83d52a18508d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = sample_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4c785043-8644-4c76-9ce4-35de706270e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "86f72464-9762-4320-a2a9-0df5ce46c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([sample_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "740bdf67-5e9e-40e2-8da0-7c5c1e52ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(sample_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9cccc32d-c10a-4bb0-9383-83c45bfbeec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 5490])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95a488-5489-40da-98d1-be99c9d9781a",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99e249b3-c5df-4587-892f-8cd28e10235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fd436a5f-cf7a-4448-b6db-d3e7fdccf23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5490])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157f6b62-d6cc-4ece-a347-9cc2d08073da",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f8ed8396-411d-417c-a3b1-117f100608f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5490])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98467f5-058a-4e2c-8089-6cdcc0e18141",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "504c3590-28df-45ee-882a-7b15d7ccbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5266ef3b-7449-4604-90a1-e958c4846cc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 13], device='cuda:0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f4d638a3-9c6c-4874-8d8b-1586689e6d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1f5bd53b-62fb-4f6c-8f0a-09754d6cf786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<NUMBER>\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5c9acdc7-6e17-4e21-8bea-cbaedb6c69bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 13], device='cuda:0')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c380eb5-701c-4c64-b247-0d91d7746072",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
